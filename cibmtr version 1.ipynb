{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":70942,"databundleVersionId":10381525,"sourceType":"competition"},{"sourceId":211322530,"sourceType":"kernelVersion"}],"dockerImageVersionId":30918,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install /kaggle/input/pip-install-lifelines/autograd-1.7.0-py3-none-any.whl\n!pip install /kaggle/input/pip-install-lifelines/autograd-gamma-0.5.0.tar.gz\n!pip install /kaggle/input/pip-install-lifelines/interface_meta-1.3.0-py3-none-any.whl\n!pip install /kaggle/input/pip-install-lifelines/formulaic-1.0.2-py3-none-any.whl\n!pip install /kaggle/input/pip-install-lifelines/lifelines-0.30.0-py3-none-any.whl","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-05T14:10:16.681053Z","iopub.execute_input":"2025-03-05T14:10:16.681298Z","iopub.status.idle":"2025-03-05T14:10:42.833502Z","shell.execute_reply.started":"2025-03-05T14:10:16.681276Z","shell.execute_reply":"2025-03-05T14:10:42.832329Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv(\"/kaggle/input/equity-post-HCT-survival-predictions/train.csv\",index_col='ID')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T14:11:39.074950Z","iopub.execute_input":"2025-03-05T14:11:39.075412Z","iopub.status.idle":"2025-03-05T14:11:39.312570Z","shell.execute_reply.started":"2025-03-05T14:11:39.075374Z","shell.execute_reply":"2025-03-05T14:11:39.311307Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"threshold = 1.0  # 30%\n\n# Calculate missing value percentage for each column\nmissing_percent = df.isnull().mean()\n\n# Identify columns to drop\ncolumns_to_drop = missing_percent[missing_percent > threshold].index.tolist()\n\n# Print removed columns\nprint(\"Columns removed due to missing values > 50%:\")\nprint(columns_to_drop)\n\n# Drop the columns\ndata_cleaned = df.drop(columns=columns_to_drop)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T14:11:48.102307Z","iopub.execute_input":"2025-03-05T14:11:48.102655Z","iopub.status.idle":"2025-03-05T14:11:48.174510Z","shell.execute_reply.started":"2025-03-05T14:11:48.102626Z","shell.execute_reply":"2025-03-05T14:11:48.172995Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.experimental import enable_iterative_imputer  # Enable Iterative Imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\n\ndef pmm_imputation(df):\n\n    df_imputed = df.copy()  # Create a copy to avoid modifying the original dataset\n\n    # ðŸ”¹ Step 1: Identify categorical & numerical columns\n    cat_cols = df_imputed.select_dtypes(include=['object']).columns.tolist()\n    num_cols = df_imputed.select_dtypes(include=['number']).columns.tolist()\n\n    # ðŸ”¹ Step 2: Remove constant (zero-variance) columns\n    for col in df_imputed.columns:\n        if df_imputed[col].nunique() == 1:\n            df_imputed.drop(columns=[col], inplace=True)\n\n    # ðŸ”¹ Step 3: Encode categorical columns using Label Encoding\n    label_encoders = {}\n    for col in cat_cols:\n        le = LabelEncoder()\n        df_imputed[col] = df_imputed[col].astype(str)  # Convert to string to avoid NaN issues\n        df_imputed[col] = le.fit_transform(df_imputed[col])\n        label_encoders[col] = le  # Store label encoders for decoding later\n\n    # ðŸ”¹ Step 4: Scale numerical columns (to prevent instability in PMM)\n    scaler = StandardScaler()\n    df_imputed[num_cols] = scaler.fit_transform(df_imputed[num_cols])\n\n    # ðŸ”¹ Step 5: Apply PMM (Predictive Mean Matching) via IterativeImputer\n    print(\"\\nâš¡ Applying Predictive Mean Matching (PMM) Imputation...\")\n    pmm_imputer = IterativeImputer(sample_posterior=True, max_iter=10, random_state=42, min_value=0)\n    df_imputed[:] = pmm_imputer.fit_transform(df_imputed)  # Apply imputation to the entire dataset\n\n    # ðŸ”¹ Step 6: Restore categorical columns to original encoding\n    for col in cat_cols:\n        df_imputed[col] = df_imputed[col].round().astype(int)  # Ensure integer values\n        df_imputed[col] = label_encoders[col].inverse_transform(df_imputed[col])  # Decode labels back to original categories\n\n    # ðŸ”¹ Step 7: Reverse scaling for numerical columns\n    df_imputed[num_cols] = scaler.inverse_transform(df_imputed[num_cols])\n\n    print(\"\\nâœ… PMM Imputation Complete!\")\n    return df_imputed\n\nimputed_pmm = pmm_imputation(data_cleaned)  # Apply PMM imputation\n\n# Check if missing values are handled\nprint(\"\\nMissing Values After Imputation:\")\nprint(imputed_pmm.isnull().sum().sum())  # Should print 0 if all missing values are handled\n\n# Save the imputed dataset\n# imputed_pmm.to_csv(\"imputed_data.csv\", index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T14:11:51.958841Z","iopub.execute_input":"2025-03-05T14:11:51.959354Z","iopub.status.idle":"2025-03-05T14:12:41.519065Z","shell.execute_reply.started":"2025-03-05T14:11:51.959316Z","shell.execute_reply":"2025-03-05T14:12:41.517909Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# IMPORTS FINAL\nimport pandas as pd\nimport numpy as np\nimport lifelines\nfrom lifelines import NelsonAalenFitter\nimport lightgbm as lgb\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\n# Load data\n# train = pd.read_csv('/kaggle/input/equity-post-HCT-survival-predictions/train.csv', index_col='ID')\ntrain= imputed_pmm.copy()\ntest = pd.read_csv('/kaggle/input/equity-post-HCT-survival-predictions/test.csv', index_col='ID')\ndata_description = pd.read_csv('/kaggle/input/equity-post-HCT-survival-predictions/data_dictionary.csv')\n\n# Take 85% of the data for training uniformly\ntrain_split = train.sample(frac=0.85, random_state=42)\n\n# Categorical & Numeric columns\ncat_cols = []\nnum_cols = []\nfor v, t in data_description[['variable', 'type']].values:\n    if t == 'Categorical' and v != 'efs':\n        cat_cols.append(v)\n    elif v not in ['efs_time', 'efs']:\n        num_cols.append(v)\n\n# Feature engineering using Nelson-Aalen estimator for training set\nnaf = NelsonAalenFitter()\nnaf.fit(train_split['efs_time'], train_split['efs'])\ntrain_split['naf_label'] = -naf.cumulative_hazard_at_times(train_split['efs_time']).values\ntrain_split.loc[train_split['efs'] == 0, 'naf_label'] -= 0.15\n\n# Model Parameters for LightGBM\nlgbm_naf_params = {\n    'task': 'train',\n    'objective': 'regression',\n    'boosting_type': 'gbdt',\n    'learning_rate': 0.05,\n    'num_leaves': 10,\n    'max_depth': 5,\n    'min_data_in_leaf': 10,\n    'min_sum_hessian_in_leaf': 1e-3,\n    'feature_fraction': 1.0,\n    'bagging_fraction': 1.0,\n    'bagging_freq': 1,\n    'lambda_l1': 0.0,\n    'lambda_l2': 0.0,\n    'metric': 'rmse',\n    'seed': 53,\n    'n_estimators': 1999,\n    'num_threads': 4,\n    'device_type': 'cpu'\n}\n\n# Define target columns\ntarget_cols = ['efs', 'efs_time', 'naf_label']\n\n# Convert object type features to category for proper handling\ncat_cols = train_split.drop(columns=target_cols).select_dtypes(include='object').columns.tolist()\ntrain_split[cat_cols] = train_split[cat_cols].astype('category')\ntest[cat_cols] = test[cat_cols].astype('category')\n\n# Prepare LightGBM datasets\ntrain_lgb_naf = lgb.Dataset(train_split.drop(columns=target_cols),\n                            label=train_split['naf_label'],\n                            categorical_feature=cat_cols)\n\n# Train LightGBM model\nbest_naf = lgb.train(lgbm_naf_params, train_lgb_naf, 1000)\n\n# Prepare test data for prediction\ntest_features = test.copy()\n\n# Align columns between train_split and test\ntest_features = test_features[train_split.drop(columns=target_cols).columns]\n\n# Make predictions on test data\npreds_lgb_naf = best_naf.predict(test_features)\n\n# Prepare final submission DataFrame\nsubmission = pd.DataFrame({\n    'ID': test.index,\n    'prediction': preds_lgb_naf\n})\n\n# Save the submission to a CSV file\nsubmission.to_csv('submission.csv', index=False)\n\n# # Print the required format output\n# print(\"ID\\nprediction\")\n# for idx, pred in submission.iterrows():\n#     print(f\"{int(pred['ID'])}\\n{pred['prediction']}\")\n# print(\"total values\")\n# print(len(submission))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T14:13:29.307275Z","iopub.execute_input":"2025-03-05T14:13:29.307643Z","iopub.status.idle":"2025-03-05T14:13:36.862296Z","shell.execute_reply.started":"2025-03-05T14:13:29.307617Z","shell.execute_reply":"2025-03-05T14:13:36.861393Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T13:18:15.540517Z","iopub.execute_input":"2025-03-05T13:18:15.540949Z","iopub.status.idle":"2025-03-05T13:18:15.570649Z","shell.execute_reply.started":"2025-03-05T13:18:15.540899Z","shell.execute_reply":"2025-03-05T13:18:15.569098Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
